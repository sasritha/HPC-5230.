{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZsHxyWjalyaG",
        "outputId": "2d03a767-18ca-430c-c705-f2d5c654e720"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training model [4] on 1000 rows - 1 hidden layer\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
            "Train Error: 0.1423, Val Error: 0.1387, Time: 4.61s\n",
            "\n",
            "Training model [4, 4] on 1000 rows - 2 hidden layers\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
            "Train Error: 0.0681, Val Error: 0.0724, Time: 5.14s\n",
            "\n",
            "Training model [4] on 10000 rows - 1 hidden layer\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "Train Error: 0.0432, Val Error: 0.0402, Time: 16.11s\n",
            "\n",
            "Training model [4, 4] on 10000 rows - 2 hidden layers\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "Train Error: 0.0137, Val Error: 0.0135, Time: 15.08s\n",
            "\n",
            "Training model [4] on 100000 rows - 1 hidden layer\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "Train Error: 0.0026, Val Error: 0.0026, Time: 146.00s\n",
            "\n",
            "Training model [4, 4] on 100000 rows - 2 hidden layers\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step  \n",
            "Train Error: 0.0020, Val Error: 0.0020, Time: 139.29s\n",
            "\n",
            "Final Results Table:\n",
            "                          Dataset  Layers  Train Error  Val Error    Time (s)\n",
            "0     1000 rows - 1 hidden layer     [4]     0.142268   0.138698    4.611115\n",
            "1    1000 rows - 2 hidden layers  [4, 4]     0.068090   0.072374    5.142219\n",
            "2    10000 rows - 1 hidden layer     [4]     0.043241   0.040219   16.112213\n",
            "3   10000 rows - 2 hidden layers  [4, 4]     0.013719   0.013547   15.076488\n",
            "4   100000 rows - 1 hidden layer     [4]     0.002596   0.002600  146.001660\n",
            "5  100000 rows - 2 hidden layers  [4, 4]     0.002023   0.002019  139.287077\n"
          ]
        }
      ],
      "source": [
        "#  Import the libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# Function to train a model\n",
        "def train_model(X, y, hidden_layers, dataset_name):\n",
        "    print(f\"\\nTraining model {hidden_layers} on {dataset_name}\")\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_train = scaler.fit_transform(X_train)\n",
        "    X_val = scaler.transform(X_val)\n",
        "\n",
        "    model = Sequential()\n",
        "    for i, units in enumerate(hidden_layers):\n",
        "        if i == 0:\n",
        "            model.add(Dense(units, activation='relu', input_shape=(X_train.shape[1],)))\n",
        "        else:\n",
        "            model.add(Dense(units, activation='relu'))\n",
        "    model.add(Dense(1))  # Output layer for regression\n",
        "\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "    start = time.time()\n",
        "    model.fit(X_train, y_train, epochs=20, batch_size=32, verbose=0, validation_data=(X_val, y_val))\n",
        "    end = time.time()\n",
        "\n",
        "    train_pred = model.predict(X_train)\n",
        "    val_pred = model.predict(X_val)\n",
        "    train_error = mean_squared_error(y_train, train_pred)\n",
        "    val_error = mean_squared_error(y_val, val_pred)\n",
        "    time_taken = end - start\n",
        "\n",
        "    print(f\"Train Error: {train_error:.4f}, Val Error: {val_error:.4f}, Time: {time_taken:.2f}s\")\n",
        "    return {\n",
        "        \"Dataset\": dataset_name,\n",
        "        \"Layers\": str(hidden_layers),\n",
        "        \"Train Error\": train_error,\n",
        "        \"Val Error\": val_error,\n",
        "        \"Time (s)\": time_taken\n",
        "    }\n",
        "\n",
        "# Dataset filenames\n",
        "files = {\n",
        "    \"1000\": \"dataset_1000.csv\",\n",
        "    \"10000\": \"dataset_10000.csv\",\n",
        "    \"100000\": \"dataset_1e+05.csv\"\n",
        "}\n",
        "\n",
        "results = []\n",
        "\n",
        "# Loop through datasets and train models\n",
        "for size, filename in files.items():\n",
        "    try:\n",
        "        data = pd.read_csv(filename)\n",
        "        if \"target\" in data.columns:\n",
        "            y = data[\"target\"]\n",
        "            X = data.drop(columns=[\"target\"])\n",
        "        else:\n",
        "            # Assume last column is target if not named\n",
        "            y = data.iloc[:, -1]\n",
        "            X = data.iloc[:, :-1]\n",
        "\n",
        "        results.append(train_model(X, y, [4], f\"{size} rows - 1 hidden layer\"))\n",
        "        results.append(train_model(X, y, [4, 4], f\"{size} rows - 2 hidden layers\"))\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"File not found: {filename} (please ensure it's in your directory)\")\n",
        "\n",
        "# Save results\n",
        "df_results = pd.DataFrame(results)\n",
        "df_results.to_csv(\"deep_learning_results.csv\", index=False)\n",
        "print(\"\\nFinal Results Table:\\n\", df_results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The results table shows how neural network performance varies with dataset size and model complexity. As the number of rows increases from 1,000 to 100,000, both training and validation errors decrease significantly, highlighting the positive impact of larger datasets on model accuracy. For example, the validation error drops from around 13.9% with 1,000 rows to as low as 0.20% with 100,000 rows, showing that more data helps the model learn patterns more effectively and generalize better. Additionally, increasing model complexity by using two hidden layers instead of one consistently improves performance across all dataset sizes. This is especially noticeable in smaller datasets, where the validation error is nearly halved when a second hidden layer is added. However, the performance gain becomes smaller as the dataset size increases, suggesting diminishing returns from added complexity when ample data is available. In terms of training time, it increases substantially with dataset size, but the number of layers has only a minor effect. Interestingly, for the largest dataset, the two-layer model trained slightly faster than the one-layer model, possibly due to variation in system performance or optimization differences. Overall, the results emphasize the importance of both data quantity and appropriate model complexity for achieving low error rates in neural network training.\n"
      ],
      "metadata": {
        "id": "hrHXpTIovC1l"
      }
    }
  ]
}